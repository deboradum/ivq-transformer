vqvae:
  encoder_h_dim: 128
  res_h_dim: 64
  num_res_layers: 2
  k: 1024
  d: 64
  beta: 0.25
  pretrain_path: "1024_vqvae.pth"
transformer:
  num_heads: 4
  num_layers: 4
  num_classes: 1000
  use_rms_norm: true
geotransformer:
  pretrain_path: "none"
train:
  wandb_optimize: true
  wandb_log: true
  fp16: true
  num_epochs: 5
  patience: 5
  dataset_name: "imagenet256"
  batch_size: 20
  optimizer: "adam"
  learning_rate: 0.0001
  weight_decay: 0.0001
  log_interval: 250
  save_interval: -1
