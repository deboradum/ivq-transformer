vqvae:
  encoder_h_dim: 128
  res_h_dim: 64
  num_res_layers: 2
  k: 512
  d: 64
  beta: 0.25
  pretrain_path: "checkpoint_epoch_7.pth"
transformer:
  num_heads: 4
  num_layers: 4
  num_classes: 102
  use_rms_norm: true
geotransformer:
  pretrain_path: "none"
train:
  num_epochs: 10
  dataset_name: "caltech101"
  batch_size: 32
  optimizer: "adam"
  learning_rate: 0.0001
  weight_decay: 0.0001
  log_interval: 10
  save_interval: 1
